---
title: "NVDA_S&P500"
author: "Elena"
date: "`r Sys.Date()`"
output: 
  github_document:
    toc: true
    fig_width: 6
    fig_height: 4
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE, 
  message = FALSE, 
  warning = FALSE,
  fig.path = "plots/"  
)
```

## About this project

In this project, we explore and model the volatility of NVIDIA's (NVDA) daily stock returns using ARCH and GARCH models. The analysis includes exploratory data analysis (EDA), stationarity and normality tests, and fitting time-varying volatility models to better understand the risk and dynamics of NVIDIA’s return series.

## Research Question

Which volatility model best captures the time-varying and persistent volatility in NVIDIA’s daily stock returns: ARCH or GARCH?

## Dataset description

The dataset used in this project contains daily adjusted closing prices of NVIDIA (NVDA) stock obtained from Yahoo Finance using the quantmod R package.

- **Time period covered:** January 3, 2013 to December 29, 2024

- **Frequency:** Daily (trading days)

- **Total observations:** 3,035 daily prices

- **Variable of interest:** Daily log returns computed from adjusted closing prices

- **Missing values:** None after filtering and cleaning

This dataset is well-suited for financial time series modeling as it captures volatility clustering, heavy tails, and high persistence in returns, all of which are characteristic of equity market behavior.

**Research question:** Какая модель наиболее точно описывает изменения во времени волатильности и взаимосвязь между nvidia и индексом s&p 500?

## EDA

```{r libraries}
# Install missing packages if not already installed
required_packages <- c("quantmod", "ggplot2", "PerformanceAnalytics", "tseries", 
                       "fGarch", "zoo", "rugarch", "rmarkdown", "patchwork", "FinTS")

new_packages <- required_packages[!(required_packages %in% installed.packages()[,"Package"])]
if(length(new_packages)) install.packages(new_packages)
# Load all libraries
library(patchwork)
library(quantmod)
library(ggplot2)
library(PerformanceAnalytics)
library(tseries)
library(fGarch)
library(zoo)
library(rugarch)
library(rmarkdown)
library(xts)
library(FinTS)
options(digits=4)
```

Data loading

```{r pressure, echo=FALSE}
symbol.vec <- c("NVDA", "^GSPC")
beg <- "2013-01-01"
end <- "2024-12-31"
getSymbols(symbol.vec, from=beg, to=end)
colnames(NVDA)
start(NVDA)
end(NVDA)
colnames(GSPC)
start(GSPC)
end(GSPC)
# extract adjusted closing prices
span <- paste0(beg,"/",end)
NVDA <- NVDA[span, "NVDA.Adjusted", drop=F]
GSPC <- GSPC[span, "GSPC.Adjusted", drop=F]
X <- data.frame(Date=index(NVDA), NVDA=coredata(NVDA), GSPC=coredata(GSPC))
head(X)
```
We uploaded libraries and downloaded the data for the period 2013-01-01 - 2024-12-31. Let's build lineplots for Nvidia and S&P500 adjusted closing prices
```{r, echo=FALSE}
p1 <- ggplot(data = X, aes(x = Date)) + 
  geom_line(aes(y = NVDA.Adjusted), color = "steelblue", linewidth = 1) +
  labs(title = "Nvidia (NVDA) Adjusted Closing Prices",
       x = "Date", 
       y = "Price (USD)") +
  theme_minimal(base_size = 12) +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))

# Improved S&P 500 Plot
p2 <- ggplot(data = X, aes(x = Date)) + 
  geom_line(aes(y = GSPC.Adjusted), color = "darkred", linewidth = 1) +
  labs(title = "S&P 500 Adjusted Closing Prices",
       x = "Date", 
       y = "Price (USD)") +
  theme_minimal(base_size = 12) +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))
# Combine Plots Vertically
p1 / p2
```
We convert adj closing prices to log returns to make the data stationary: prices tend to drift over time while log returns remain stable.

```{r log-returns calculation}

# Calculating log returns, removing NA, combining log returns into a data frame
# Step 1: Log returns using xts (not X$NVDA!)
NVDA.ret <- diff(log(NVDA))  # NVDA is xts
GSPC.ret <- diff(log(GSPC))  # GSPC is xts

# Step 2: Merge returns as xts
NVDA.GSPC.ret <- na.omit(merge(NVDA.ret, GSPC.ret))

# Step 3: Convert to data frame with proper dates
X <- data.frame(Date = index(NVDA.GSPC.ret), coredata(NVDA.GSPC.ret))
colnames(X)[2:3] <- c("NVDA_lr", "GSPC_lr")

# Check structure
str(X)
# Log returns Plot
p<-ggplot(data = X, aes(x = Date)) +
  geom_line(aes(y = NVDA_lr, color = "NVDA")) +
  geom_line(aes(y = GSPC_lr, color = "GSPC")) +
  labs(title = "log returns: NVDA vs S&P 500",
       x = "date", y = "log returns") +
  scale_color_manual(values = c("NVDA" = "steelblue", "GSPC" = "darkred")) +
  theme_minimal()
print(p)

```
In this chunk we are fitting a linear regression without an intercept to model the relationship between the log returns of NVDA and the S&P 500. What do we see here?

- Moderate Linear Dependence:
About 40% of the variability in the S&P 500’s log returns can be explained by the log returns of NVDA. This suggests a moderate co-movement between the two assets.

- Beta = 0.23903. As beta < 1, NVDA’s movements explain only a small fraction of S&P 500’s return movements.

NVDA moves less than the market.

- The correlation is moderate, as shown by your R^2 = 0.405, only about 40% of the variation in GSPC returns can be explained by NVDA's returns.

- Portfolio Implication:
NVDA does not move perfectly with the market meaning diversification benefits exist. But because 40% is explained, NVDA has a meaningful market exposure.

- Risk Modeling Use:
You now know that NVDA and S&P 500 share moderate systematic risk. That info is useful for covariance estimation, Value-at-Risk, or building a two-asset hedge.

```{r linear regression}
# Regression without intercept on log returns:
require("ggpmisc")

# Regression formula without intercept
reg <- y ~ -1 + x  

# Scatter plot with regression line and equation
p <- ggplot(data = X, aes(x = NVDA_lr, y = GSPC_lr)) +
  ggtitle("Log Returns: NVDA vs GSPC with Linear Regression Fit") +
  labs(x = "NVDA Log Returns", y = "GSPC Log Returns") +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = FALSE, color = "red", formula = reg) +
  stat_poly_eq(formula = reg,
               eq.with.lhs = "italic(hat(y))~`=`~",
               aes(label = paste(..eq.label.., ..rr.label.., sep = "*plain(\",\")~")),
               parse = TRUE) +
  theme(plot.title = element_text(hjust = 0.5))

print(p)

summary(lm(GSPC_lr ~ NVDA_lr - 1, data = X))
cor(X$NVDA, X$GSPC)^2

```
**ADF test**checks if a time series is non-stationary due to a unit root.
- Null Hypothesis (H₀): Series has a unit root → non-stationary
- Alternative Hypothesis (H₁): Series is stationary
Results:
Because the p-values are < 0.05, we reject H₀ and conclude that both NVDA and GSPC log returns are stationary.

**KPSS test**also checks for stationarity.
- Null Hypothesis (H₀): Series is stationary
- Alternative Hypothesis (H₁): Series is non-stationary

Results: The p-values > 0.05, so we fail to reject H₀ and conclude the series are stationary.

**Jarque Bera**tests whether the data follows a normal distribution.
- Null Hypothesis (H₀): Data is normally distributed
- Alternative Hypothesis (H₁: Data is not normally distributed
Results: The p-values < 0.05, so we reject H₀ and conclude the series are not normally distributed.

```{r ADF, KPSS, normality}
# Rename columns for clarity (optional, your data already has these names)
colnames(X)[2:3] <- c("NVDA_lr", "GSPC_lr")

# Stationarity Tests
# Augmented Dickey-Fuller Test (ADF)
# H0: Series has a unit root (non-stationary)
adf_nvda <- adf.test(X$NVDA_lr, alternative = "stationary")
adf_gspc <- adf.test(X$GSPC_lr, alternative = "stationary")
print(adf_nvda)
print(adf_gspc)

# KPSS Test
# H0: Series is stationary
kpss_nvda <- kpss.test(X$NVDA_lr, null = "Level")
kpss_gspc <- kpss.test(X$GSPC_lr, null = "Level")
print(kpss_nvda)
print(kpss_gspc)

# PART 5: Normality Test (Jarque-Bera)
jb_nvda <- jarque.bera.test(X$NVDA_lr)
jb_gspc <- jarque.bera.test(X$GSPC_lr)
print(jb_nvda)
print(jb_gspc)

```
The ACF and PACF plots for NVDA log returns show weak autocorrelation with only a few significant spikes, suggesting limited serial dependence and a pattern close to white noise. In contrast, S&P 500 returns exhibit more pronounced autocorrelation with several significant spikes in both ACF and PACF, indicating the presence of short-term dependencies. 

```{r ACF/PACF plots}
# PART 4: Autocorrelation Analysis
par(mfrow = c(2, 2))  # 2x2 plot layout
acf(X$NVDA_lr, main = "ACF: NVDA Log Returns")
pacf(X$NVDA_lr, main = "PACF: NVDA Log Returns")
acf(X$GSPC_lr, main = "ACF: GSPC Log Returns")
pacf(X$GSPC_lr, main = "PACF: GSPC Log Returns")
par(mfrow = c(1, 1))  # Reset layout

```
Markets are dynamic and relationships between NVDA and the S&P 500 may change over time. Static correlation may miss periods of high or low correlation, shifts in volatility regimes or co-movement behavior.

Instead of calculating one correlation value over the whole dataset, we calculate it over a moving window (60 trading days or 3 months). We see that there are periods when correlation was high (near 1) meaning NVDA and S&P 500 moved together. No negative correlation but somtimes correlation dropped to 0 indicating that NVDA and S&P diverged. There is a lot of fluctuation in the plot, their relationship is not constant and depends on market conditions. 

```{r Rolling Covariance and Correlation}
# PART 6: Rolling Covariance and Correlation
# Convert to xts object
NVDA.GSPC.ret.xts <- xts(X[, c("NVDA_lr", "GSPC_lr")], order.by = X$Date)

# Rolling 60-day window
rolling_cov <- rollapply(NVDA.GSPC.ret.xts, width = 60, FUN = function(x) cov(x[,1], x[,2]),
                         by.column = FALSE, align = "right")

rolling_cor <- rollapply(NVDA.GSPC.ret.xts, width = 60, FUN = function(x) cor(x[,1], x[,2]),
                         by.column = FALSE, align = "right")

# Prepare for ggplot
rolling_cor_df <- data.frame(Date = index(rolling_cor), RollingCorrelation = coredata(rolling_cor))
p5 <- ggplot(rolling_cor_df, aes(x = Date, y = RollingCorrelation)) + 
  geom_line(color = "purple") + 
  ggtitle("60-Day Rolling Correlation: NVDA vs GSPC") +
  labs(x = "Date", y = "Rolling Correlation") +
  theme_minimal()

print(p5)
```
It is time to perform ARCH tests to see whether volatility clusters over time: if periods of high volatility tend to follow each other, and same for low volatility.

- Null Hypothesis (H₀): No ARCH effect (constant variance).

- Alternative Hypothesis (H₁): ARCH effect exists (variance depends on past squared residuals).

As the p-value < 0.05, we reject H₀.There is time-varying volatility and we fit a GARCH model.


```{r ARCH tests} 
ArchTest(X$NVDA_lr, lags = 5)
ArchTest(X$GSPC_lr, lags = 5)

# If significant (p-value < 0.05), proceed to fit GARCH(0,5)
```
First, we fit GARCH(0,5) model which is equivalent to ARCH(0,5) because we want to diagnose how many ARCH lags are significant. 
**NVDA**
- All ARCH coefficients (a1 to a5) are statistically significant (p < 0.05).

- Sum of ARCH coefficients = 0.6098. It indicates moderate persistence of volatility.

Diagnostics:

- Jarque-Bera test: residuals not normally distributed (p < 2e-16).

- Box-Ljung test on squared residuals: p = 0.7, no significant autocorrelation left, model fits conditional variance well.

**GSPC**
- All ARCH coefficients are highly significant.

- Sum of ARCH coefficients: 0.7893 indicating stronger persistence of volatility than NVDA.

Diagnostics:

- Jarque-Bera: residuals still not normal (very common in financial time series).

- Box-Ljung: p = 0.6, again suggests residuals are white noise.Model is appropriate.

```{r GARCH(0,5)}
# GARCH(0,5) model for NVDA log returns
m_nvda <- garch(X$NVDA_lr, order = c(0, 5), trace = FALSE)
summary(m_nvda)

# Sum of ARCH coefficients (alpha1 to alpha5)
sum(coef(m_nvda)[2:length(coef(m_nvda))])

# GARCH(0,5) model for GSPC log returns
m_gspc <- garch(X$GSPC_lr, order = c(0, 5), trace = FALSE)
summary(m_gspc)

# Sum of ARCH coefficients (alpha1 to alpha5)
sum(coef(m_gspc)[2:length(coef(m_gspc))])
```
The next model is GARCH(1,1)
**NVDA:**
mu = 0.0026: Average daily return ~ 0.26%

omega = 4.0688e-05: Long-term volatility (constant)

alpha1 = 0.1173: Impact of yesterday’s shock

beta1 = 0.8399: Impact of yesterday’s volatility

Alpha + Beta = 0.9572

This means 95.7% of current volatility comes from past variance and past shocks — very persistent volatility.

Alpha + beta < 1, the process is stationary in variance.

Diagnostics:
Jarque-Bera and Shapiro-Wilk p-values = 0. Residuals are not normally distributed.

Ljung-Box on residuals and squared residuals: p-values > 0.05 It means no significant autocorrelation remains

LM ARCH test: p = 0.97 No ARCH effects left in residuals (model captured volatility clustering well).

**GSPC:**
mu = 0.0008: Much lower average daily return 

alpha1 = 0.18998, beta1 = 0.7720

Alpha + Beta = 0.962 Also high volatility persistence, slightly higher than NVDA

Same diagnostic pattern: good model fit, residuals not normal (common), no remaining ARCH effect.

```{r GARCH(1,1)}
# GARCH(1,1) model for NVDA log returns
garch_nvda <- garchFit(~ garch(1, 1), data = X$NVDA_lr, trace = FALSE)
summary(garch_nvda)

# Sum of alpha1 (ARCH) and beta1 (GARCH)
apb_nvda <- sum(coef(garch_nvda)[c("alpha1", "beta1")])
print(paste("Alpha + Beta (NVDA):", round(apb_nvda, 4)))

# GARCH(1,1) model for GSPC log returns
garch_gspc <- garchFit(~ garch(1, 1), data = X$GSPC_lr, trace = FALSE)
summary(garch_gspc)

# Sum of alpha1 and beta1
apb_gspc <- sum(coef(garch_gspc)[c("alpha1", "beta1")])
print(paste("Alpha + Beta (GSPC):", round(apb_gspc, 4)))
```
Let's create a function to implement the Exponentially Weighted Moving Average (EWMA) covariance matrix. We will use it to track the changing covariance between assets over time, putting more weight on recent observations.


This function:

- Takes a matrix/data frame X of asset returns (e.g., NVDA and GSPC daily log returns).

- Initializes a 3D array S[n, k, k] to store one covariance matrix per time point.

- Seeds the first covariance matrix using cov(X[1:5, ]) (just a starting guess).

- Computes EWMA recursively: Each new covariance matrix is a weighted average of: the previous day's covariance matrix (S[i - 1, , ]) today's outer product of returns (tcrossprod(X[i, ])). The weight is controlled by lambda (typically close to 1, like 0.94 or 0.97).

```{r cov_ewma_manual}
# New manual function:
cov_ewma_manual <- function(X, lambda = 0.94) {
  n <- nrow(X)
  k <- ncol(X)
  S <- array(NA, dim = c(n, k, k))
  S[1, , ] <- cov(X[1:5, ])  # seed with initial sample cov
  
  for (i in 2:n) {
    S[i, , ] <- lambda * S[i - 1, , ] + (1 - lambda) * tcrossprod(X[i, ])
  }
  
  return(S)
}

```
We calculate and visualize the Exponential Weighted Moving Average (EWMA) of covariance and correlation between NVDA and S&P 500 (GSPC) log returns over time.

The EWMA analysis with λ = 0.94 reveals that the average short-term covariance between NVDA and S&P 500 log returns is approximately 0.000192, while the average short-term correlation is around 0.583. This suggests a moderately strong positive co-movement between the two assets over time, with recent data weighted more heavily in the estimation.
```{r Rolling Covariance/Correlation and EWMA}
X$NVDA_lr <- X$NVDA_lr - mean(X$NVDA_lr)
X$GSPC_lr <- X$GSPC_lr - mean(X$GSPC_lr)
# Extract the return matrix
returns_mat <- as.matrix(X[, c("NVDA_lr", "GSPC_lr")])
lambda <- 0.94

# Run EWMA
cov_ewma_array <- cov_ewma_manual(returns_mat, lambda = lambda)

# Number of observations
n <- dim(cov_ewma_array)[1]

# EWMA covariance and correlation vectors
ewma_cov <- numeric(n)
ewma_cor <- numeric(n)

for (i in 1:n) {
  cov_matrix <- cov_ewma_array[i, , ]
  ewma_cov[i] <- cov_matrix[1, 2]
  ewma_cor[i] <- cov2cor(cov_matrix)[1, 2]
}

ewma_dates <- X$Date

df_cov <- data.frame(Date = ewma_dates, Covariance = ewma_cov)
df_cor <- data.frame(Date = ewma_dates, Correlation = ewma_cor)

# Plot covariance
xbar_cov <- mean(df_cov$Covariance, na.rm = TRUE)
p_cov <- ggplot(df_cov, aes(x = Date, y = Covariance)) +
  geom_line() +
  geom_hline(yintercept = xbar_cov, color = "red") +
  ggtitle(paste0("EWMA Covariance: NVDA vs GSPC (λ = ", lambda, ")\nMean = ", round(xbar_cov, 6))) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))

# Plot correlation
xbar_cor <- mean(df_cor$Correlation, na.rm = TRUE)
p_cor <- ggplot(df_cor, aes(x = Date, y = Correlation)) +
  geom_line() +
  geom_hline(yintercept = xbar_cor, color = "red") +
  ggtitle(paste0("EWMA Correlation: NVDA vs GSPC (λ = ", lambda, ")\nMean = ", round(xbar_cor, 3))) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))

# Display side-by-side
p_cov / p_cor
```
Here we analyze how the covariance and correlation between NVDA and the S&P 500 (GSPC) log returns change over time, using a rolling 1-year window (252 trading days). This analysis helps us

- see how closely NVDA and GSPC have moved together over time.

- Spot periods of higher or lower co-movement

- Understand portfolio diversification potential: low correlation means better diversification.

We see that in the covariance plot
- the rolling covariance was low and stable until 2020 indicating NVDA has relatively independent return dynamics
- starting in 2020 we observe increased market volatility and higher co-movement due to systemic shocks. NVDA bocame more sensitive to macroeconomic factors.

The correlation plot confirms that 
- before 2020 NVDA and S&P500 were less correlated. NVDA stock price was more influenced by company-specific factors than general market trends.

- starting in 2020, NVDA has become more synchronized with the market. This means NVDA now moves more in line with the broader market, reducing its role as a diversification tool in portfolios.

```{r cov and cor using 1-year-window}
# Step 1: Prepare the rolling window data
width <- 252  # One year

X$Date <- as.Date(as.character(X$Date))

# Convert to zoo for rolling calculation
NVDA.GSPC.ret.zoo <- zoo(X[, c("NVDA_lr", "GSPC_lr")], order.by = X$Date)


# Define rolling functions
cor.fun <- function(x) cor(x)[1,2]
cov.fun <- function(x) cov(x)[1,2]

# Rolling window
roll.cov <- rollapply(NVDA.GSPC.ret.zoo, FUN = cov.fun, width = width, by.column = FALSE, align = "right")
roll.cor <- rollapply(NVDA.GSPC.ret.zoo, FUN = cor.fun, width = width, by.column = FALSE, align = "right")

# Plots for Rolling Covariance
z <- data.frame(x = time(roll.cov), y = coredata(roll.cov))
xbar <- mean(roll.cov)
p1.5.1 <- ggplot(z, aes(x = x, y = y)) +
  geom_line() +
  geom_hline(yintercept = xbar, color = "red") +
  ggtitle(paste0(width, "-day Rolling Covariances of NVDA and GSPC\n (mean = ", round(xbar, 6), ")")) +
  labs(x = "Date", y = "Covariance") +
  theme(plot.title = element_text(hjust = 0.5))

# Plots for Rolling Correlation
z <- data.frame(x = time(roll.cor), y = coredata(roll.cor))
xbar <- mean(roll.cor)
p1.5.2 <- ggplot(z, aes(x = x, y = y)) +
  geom_line() +
  geom_hline(yintercept = xbar, color = "red") +
  ggtitle(paste0(width, "-day Rolling Correlations of NVDA and GSPC\n (mean = ", round(xbar, 6), ")")) +
  labs(x = "Date", y = "Correlation") +
  theme(plot.title = element_text(hjust = 0.5))

# Display plots side-by-side
p1.5.1 / p1.5.2

```
This models the time-varying volatility as a function of past squared returns and past variance. Mean equation: ARFIMA(0,0,0). This means no autoregressive or moving average component in the return mean (just a constant mean). 
Diagnostics:

- Ljung-Box tests:
All p-values > 0.05 indicate no significant autocorrelation left in standardized or squared residuals. Model captures time dependence well.

- ARCH LM Tests:
p-values > 0.05, hence no ARCH effects left. No remaining volatility clustering after fitting.

- Nyblom Stability Test:
GSPC has a Joint Statistic = 8.6, well above critical values (1.07, 1.24, 1.6) This suggests parameter instability over time.

- Adjusted Pearson Goodness-of-Fit Test:
Extremely small p-values indicate model residuals deviate from normality. This is common in financial data. Consider trying:

Conclusion:
- Both NVDA and GSPC returns show strong volatility clustering.

- GARCH(1,1) models fit well in terms of removing autocorrelation and ARCH effects.

- The parameter stability for GSPC is questionable.

- Model fit to distribution is weak (violated normality assumption).

**Sign Bias Test**
NVDA:
All individual p-values > 0.05, but joint effect has p = 0.086 Some mild evidence of asymmetry, but not strong enough to reject symmetry at 5%

GSPC:
Sign Bias p < 0.001, Joint p < 0.001 Strong evidence of asymmetry. It suggests that positive and negative returns affect volatility differently. GARCH(1,1) may be misspecified and EGARCH for GSPC should be considered.
```{r univariate GARCH(1,1)}
garch11.spec <- ugarchspec(
  mean.model = list(armaOrder = c(0, 0)), 
  variance.model = list(garchOrder = c(1, 1), model = "sGARCH"), 
  distribution.model = "norm"
)

# Fit univariate GARCH to NVDA
m_nvda <- ugarchfit(data = NVDA.GSPC.ret[, 1], spec = garch11.spec)
(m1.7.1 <- m_nvda)

# Fit univariate GARCH to GSPC
m_gspc <- ugarchfit(data = NVDA.GSPC.ret[, 2], spec = garch11.spec)
(m1.7.2 <- m_gspc)

```
The next step is Dynamic Conditional Correlation (DCC) GARCH model, which we fit to two return series: NVDA and GSPC. DCC models are used to estimate time-varying correlations between multiple financial assets, allowing both: volatility clustering (via GARCH) and dynamic correlation evolution (via DCC)

This model confirms that:

- Volatility is highly persistent in both NVDA and GSPC

- Correlations are not constant — they change over time

- The DCC part (0.7425) shows moderate dynamic correlation, appropriate for portfolio risk management and diversification analysis

```{r DCC-GARCH(1,1)}
# DCC-GARCH(1,1) specification
dcc.garch11.spec <- dccspec(
  uspec = multispec(replicate(2, garch11.spec)), 
  dccOrder = c(1, 1), 
  distribution = "mvnorm"
)
# Fit DCC model
dcc.fit <- dccfit(dcc.garch11.spec, data = NVDA.GSPC.ret)

# Explore output
class(dcc.fit)
slotNames(dcc.fit)
names(dcc.fit@mfit)
names(dcc.fit@model)
print(dcc.fit)

# Conditional correlation dynamics
sum(coef(dcc.fit)[3:4])  # NVDA
sum(coef(dcc.fit)[7:8])  # GSPC
sum(coef(dcc.fit)[9:10]) # Joint DCC alpha + beta

# Extract time-varying covariances and correlations
cov.fit <- rcov(dcc.fit)[1, 2, ]
cor.fit <- rcor(dcc.fit)[1, 2, ]

```
Let's build DCC-GARCH visualizations:

- to see how the relationship between assets evolves over time
- Understand when market co-movement increases during crises
- Detect regime changes in correlation — useful for diversification and risk modeling

```{r}
# Align DCC covariances and correlations with dates
dcc_dates <- time(as.zoo(NVDA.GSPC.ret))[(length(time(NVDA.GSPC.ret)) - length(cov.fit) + 1):length(time(NVDA.GSPC.ret))]

# Conditional covariance
w <- data.frame(x = dcc_dates, y = cov.fit)
(xbar <- mean(w$y))
p1.8.1 <- ggplot(w, aes(x = x, y = y)) +
  geom_line() +
  geom_hline(yintercept = xbar, color = "red") +
  ggtitle(paste0("DCC Conditional Covariance (mean = ", round(xbar, 6), ")")) +
  labs(x = "Date", y = "Covariance") +
  theme(plot.title = element_text(hjust = 0.5))

# Conditional correlation
w <- data.frame(x = dcc_dates, y = cor.fit)
(xbar <- mean(w$y))
p1.8.2 <- ggplot(w, aes(x = x, y = y)) +
  geom_line() +
  geom_hline(yintercept = xbar, color = "red") +
  ggtitle(paste0("DCC Conditional Correlation (mean = ", round(xbar, 3), ")")) +
  labs(x = "Date", y = "Correlation") +
  theme(plot.title = element_text(hjust = 0.5))

# Show the plots
p1.8.1 / p1.8.2
```
If nothing changes, volatility will be at the level we see in the forecast plots.
```{r volatility forecast}
# 100-step-ahead DCC forecasts
dcc.fcst <- dccforecast(dcc.fit, n.ahead = 100)

# Extract conditional covariance and correlation forecasts
cv <- rcov(dcc.fcst)[[1]][1, 2, ]
cr <- rcor(dcc.fcst)[[1]][1, 2, ]

# Get last available date and generate future dates
last_date <- tail(index(NVDA.GSPC.ret), 1)
future_dates <- seq.Date(from = last_date + 1, by = "day", length.out = length(cv))

# Sometimes weekends are included – keep only business days if needed
# library(timeDate)
# future_dates <- as.Date(timeSequence(from = last_date + 1, length.out = 100, by = "day")@Data)
# future_dates <- future_dates[isBizday(future_dates)]

# Combine past + forecasted dates
plot_dates <- c(tail(index(NVDA.GSPC.ret), 282), future_dates)

# Conditional covariance
z_cov <- data.frame(Date = plot_dates, Covariance = c(tail(cov.fit, 282), cv))
xbar_cov <- mean(cov.fit)

p1.9.1 <- ggplot(z_cov, aes(x = Date, y = Covariance)) +
  geom_line() +
  geom_vline(xintercept = future_dates[1], color = "red") +
  geom_hline(yintercept = xbar_cov, color = "blue") +
  ggtitle(paste0("DCC Conditional Covariance Forecast (mean = ", round(xbar_cov, 6), ")")) +
  labs(x = "Date", y = "Covariance") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))

# Conditional correlation
z_cor <- data.frame(Date = plot_dates, Correlation = c(tail(cor.fit, 282), cr))
xbar_cor <- mean(cor.fit)

p1.9.2 <- ggplot(z_cor, aes(x = Date, y = Correlation)) +
  geom_line() +
  geom_vline(xintercept = future_dates[1], color = "red") +
  geom_hline(yintercept = xbar_cor, color = "blue") +
  ggtitle(paste0("DCC Conditional Correlation Forecast (mean = ", round(xbar_cor, 3), ")")) +
  labs(x = "Date", y = "Correlation") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))

# Show the plots
p1.9.1 / p1.9.2

```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
